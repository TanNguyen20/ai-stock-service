# app.py
"""
VN Stock News‚ÄìPrice Analyzer (Vietnam-native prices via vnstock)

Enter a Vietnam stock ticker (e.g., HPG, MSN, VNM, MBS) to:
  ‚Ä¢ Fetch price history from Vietnam sources (library: vnstock)
  ‚Ä¢ Pull recent Vietnamese news via Google News RSS
  ‚Ä¢ Score sentiment (lightweight lexicon; optional multilingual transformer)
  ‚Ä¢ Correlate daily sentiment vs. forward returns
  ‚Ä¢ Visualize and export CSVs

Install:
  pip install streamlit vnstock feedparser pandas numpy plotly
# Optional for stronger sentiment (larger build):
# pip install transformers torch --extra-index-url https://download.pytorch.org/whl/cpu

Run:
  streamlit run app.py
"""

import datetime as dt
import math
import re
from dataclasses import dataclass
from typing import List, Optional, Tuple, Dict, Any

import numpy as np
import pandas as pd
import streamlit as st
import plotly.graph_objects as go
import plotly.express as px
import feedparser

# Optional transformer sentiment (multilingual)
try:
    from transformers import AutoModelForSequenceClassification, AutoTokenizer  # type: ignore
    import torch  # type: ignore
    _HAS_TFM = True
except Exception:
    _HAS_TFM = False

# -------------------------- UI & Config -------------------------- #

st.set_page_config(page_title="VN Stock News‚ÄìPrice Analyzer", layout="wide")

@dataclass
class NewsItem:
    published: dt.datetime
    title: str
    summary: str
    link: str
    source: str
    sentiment: Optional[float] = None  # -1 .. 1

def _coerce_dt(x: str) -> dt.datetime:
    try:
        return dt.datetime(*feedparser._parse_date(x)[:6])  # type: ignore[attr-defined]
    except Exception:
        return dt.datetime.utcnow()

# -------------------------- VN price backend -------------------------- #

def _clean_price_df_from_vnstock(df: pd.DataFrame) -> pd.DataFrame:
    """
    vnstock.stock_historical_data returns columns like:
      time, open, high, low, close, volume (and maybe 'ticker')
    Normalize to: date, Open, High, Low, Close, Volume (float), ascending by date.
    """
    df = df.copy()
    # standardize column names (lowercase first)
    df.columns = [c.strip().lower().replace(" ", "_") for c in df.columns]

    # figure out the date/time column name
    time_col = "time" if "time" in df.columns else ("date" if "date" in df.columns else None)
    if time_col is None:
        raise ValueError("vnstock returned data without a 'time' or 'date' column.")
    df.rename(columns={time_col: "date"}, inplace=True)

    # rename OHLCV to Title case expected by Plotly
    rename_map = {
        "open": "Open",
        "high": "High",
        "low": "Low",
        "close": "Close",
        "adj_close": "Adj Close",
        "volume": "Volume",
    }
    for k, v in rename_map.items():
        if k in df.columns:
            df.rename(columns={k: v}, inplace=True)

    # ensure datetime & numeric types
    df["date"] = pd.to_datetime(df["date"]).dt.tz_localize(None)
    for col in ["Open", "High", "Low", "Close", "Adj Close", "Volume"]:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # drop bad rows and sort
    keep_cols = [c for c in ["Open", "High", "Low", "Close"] if c in df.columns]
    if keep_cols:
        df = df.dropna(subset=keep_cols)
    if "Close" in df.columns:
        df = df[df["Close"] > 0]
    df = df.sort_values("date").reset_index(drop=True)
    return df

def load_prices_vietnam(ticker: str, start: dt.date, end: dt.date) -> Tuple[str, pd.DataFrame, pd.DataFrame]:
    """
    Fetch daily OHLCV for a VN ticker using the vnstock library.
    Returns (resolved_symbol, prices_df, debug_log_df)
    """
    attempts: List[Dict[str, Any]] = []

    try:
        # import inside to keep startup fast if user hasn't installed vnstock yet
        from vnstock import stock_historical_data  # type: ignore
    except Exception as e:
        raise RuntimeError(
            "The 'vnstock' package is not installed. Please add 'vnstock' to requirements.txt."
        ) from e

    # vnstock uses 'YYYY-MM-DD' strings
    start_str = start.strftime("%Y-%m-%d")
    end_str = end.strftime("%Y-%m-%d")

    symbol = ticker.strip().upper()
    # Single attempt (vnstock doesn't require exchange suffix)
    try:
        df = stock_historical_data(
            symbol=symbol,
            start_date=start_str,
            end_date=end_str,
            resolution="1D",  # daily bars
        )
        n = int(len(df)) if isinstance(df, pd.DataFrame) else 0
        attempts.append({"symbol": symbol, "method": "vnstock.stock_historical_data(1D)", "rows": n})
        if n > 0:
            cleaned = _clean_price_df_from_vnstock(df)
            if len(cleaned) > 0:
                return symbol, cleaned, pd.DataFrame(attempts)
    except Exception as e:
        attempts.append({"symbol": symbol, "method": "vnstock ERROR", "rows": 0, "msg": str(e)})

    debug_df = pd.DataFrame(attempts)
    raise RuntimeError(
        f"No VN data returned for {symbol}. "
        "Please check the ticker code (e.g., HPG, VNM, FPT, MBS) and date range."
    )

# -------------------------- News & Sentiment -------------------------- #

def google_news_rss(query: str, days: int = 30, lang: str = "vi") -> str:
    q = re.sub(r"\s+", "+", query.strip())
    return f"https://news.google.com/rss/search?q={q}+when:{days}d&hl={lang}&gl=VN&ceid=VN:{lang}"

def fetch_news(query: str, days: int = 30) -> List[NewsItem]:
    url = google_news_rss(query, days=days)
    feed = feedparser.parse(url)
    items: List[NewsItem] = []
    for e in feed.entries:
        title = e.get("title", "").strip()
        summary = re.sub(r"<[^>]+>", " ", e.get("summary", "").strip())
        link = e.get("link", "")
        src = e.get("source", {})
        source_title = src.get("title", src) if isinstance(src, dict) else src
        published = _coerce_dt(e.get("published", ""))
        items.append(NewsItem(published, title, summary, link, source_title or ""))
    # dedup by title
    dedup = {}
    for it in items:
        key = it.title.lower()
        if key not in dedup:
            dedup[key] = it
    return sorted(dedup.values(), key=lambda x: x.published)

# Simple Vietnamese lexicon (fallback if no transformer)
VI_POS = set("tƒÉng|k·ª∑ l·ª•c|t√≠ch c·ª±c|l·ª£i nhu·∫≠n|v∆∞·ª£t|b·ª©t ph√°|kh·∫£ quan|mua r√≤ng|ƒë·ªânh|b√πng n·ªï|thu·∫≠n l·ª£i".split("|"))
VI_NEG = set("gi·∫£m|ti√™u c·ª±c|thua l·ªó|l·ªó|suy gi·∫£m|s·ª•t|b√°n r√≤ng|kh√≥ khƒÉn|ƒëi·ªÅu tra|ph·∫°t|r·ªßi ro|ƒë√¨nh ch·ªâ|suy tho√°i|n·ª£ x·∫•u".split("|"))

@st.cache_resource(show_spinner=False)
def _load_model():
    if not _HAS_TFM:
        return None, None
    try:
        name = "cardiffnlp/twitter-xlm-roberta-base-sentiment"
        tok = AutoTokenizer.from_pretrained(name)
        mdl = AutoModelForSequenceClassification.from_pretrained(name)
        mdl.eval()
        return tok, mdl
    except Exception:
        return None, None

def score_sentiment(text: str) -> float:
    text = (text or "").strip()
    if not text:
        return 0.0
    tok, mdl = _load_model()
    if tok is not None and mdl is not None:
        try:
            with torch.no_grad():  # type: ignore[attr-defined]
                inputs = tok(text[:512], return_tensors="pt", truncation=True)
                logits = mdl(**inputs).logits[0]
                probs = torch.softmax(logits, dim=0).cpu().numpy()
                return float(probs[2] - probs[0])  # pos - neg
        except Exception:
            pass
    toks = re.findall(r"\w+", text.lower())
    pos = sum(1 for t in toks if t in VI_POS)
    neg = sum(1 for t in toks if t in VI_NEG)
    if pos == 0 and neg == 0:
        return 0.0
    return (pos - neg) / max(1, pos + neg)

def attach_sentiment(items: List[NewsItem]) -> List[NewsItem]:
    for it in items:
        it.sentiment = score_sentiment(f"{it.title}. {it.summary}")
    return items

# ------------------------------ UI ------------------------------ #

st.title("üáªüá≥ VN Stock News‚ÄìPrice Analyzer")
st.caption("D·ªØ li·ªáu gi√° l·∫•y t·ª´ ngu·ªìn Vi·ªát Nam (th∆∞ vi·ªán `vnstock`). Tin t·ª©c t·ª´ Google News (ti·∫øng Vi·ªát).")

col1, col2, col3 = st.columns([2, 1, 1])
with col1:
    raw_ticker = st.text_input("M√£ c·ªï phi·∫øu (VD: HPG, VNM, MSN, FPT, MBS...)", value="HPG")
with col2:
    lookback_days = st.number_input("S·ªë ng√†y xem tin", value=30, min_value=7, max_value=365)
with col3:
    date_range = st.date_input(
        "Kho·∫£ng th·ªùi gian gi√°",
        value=(dt.date.today() - dt.timedelta(days=365), dt.date.today()),
        min_value=dt.date(2010, 1, 1),
        max_value=dt.date.today(),
    )

company_hint = st.text_input("T√™n c√¥ng ty (t√πy ch·ªçn, tƒÉng ƒë·ªô ch√≠nh x√°c t√¨m tin)", value="H√≤a Ph√°t")
run = st.button("Ph√¢n t√≠ch ngay", type="primary")

if run:
    try:
        if isinstance(date_range, (list, tuple)) and len(date_range) == 2:
            start, end = date_range
        else:
            start, end = dt.date.today() - dt.timedelta(days=365), dt.date.today()

        # --- Prices from vnstock ---
        resolved, prices, debug_log = load_prices_vietnam(raw_ticker, start, end)
        st.success(f"ƒê√£ t√¨m th·∫•y d·ªØ li·ªáu gi√°: {resolved} ({len(prices)} phi√™n)")
        with st.expander("Debug: th√¥ng tin t·∫£i gi√°"):
            st.dataframe(debug_log, use_container_width=True, hide_index=True)

        # Candlestick
        st.subheader("Gi√° l·ªãch s·ª≠")
        fig_price = go.Figure()
        fig_price.add_trace(go.Candlestick(
            x=prices["date"],
            open=prices["Open"],
            high=prices["High"],
            low=prices["Low"],
            close=prices["Close"],
            name="Gi√°",
        ))
        fig_price.update_layout(height=420, margin=dict(l=10, r=10, t=30, b=10))
        st.plotly_chart(fig_price, use_container_width=True)

        # Returns
        pxdf = prices.copy()
        pxdf["ret1"] = pxdf["Close"].pct_change()
        pxdf["ret5"] = pxdf["Close"].pct_change(5)

        # --- News + sentiment ---
        query = f"{raw_ticker} {company_hint}".strip()
        news = attach_sentiment(fetch_news(query, days=int(lookback_days)))
        if not news:
            st.warning("Kh√¥ng t√¨m th·∫•y b√†i vi·∫øt n√†o cho t·ª´ kh√≥a ƒë√£ ch·ªçn.")
        else:
            nd = pd.DataFrame([{
                "date": pd.to_datetime(n.published).tz_localize(None).date(),
                "published": pd.to_datetime(n.published).tz_localize(None),
                "title": n.title, "summary": n.summary, "link": n.link,
                "source": n.source, "sentiment": n.sentiment
            } for n in news])

            st.subheader("Tin t·ª©c & c·∫£m x√∫c th·ªã tr∆∞·ªùng")
            st.dataframe(
                nd[["published", "title", "source", "sentiment", "link"]].sort_values("published", ascending=False),
                use_container_width=True, hide_index=True
            )

            # Daily sentiment ‚Üí merge
            daily_sent = nd.groupby("date")["sentiment"].mean().reset_index().rename(columns={"sentiment": "sent_daily"})
            daily_sent = daily_sent.rename(columns={"date": "news_date"})
            pxdf["date_only"] = pd.to_datetime(pxdf["date"]).dt.date
            merged = pxdf.merge(daily_sent, left_on="date_only", right_on="news_date", how="left")
            merged["sent_daily"] = merged["sent_daily"].fillna(0.0)

            # Rolling sentiment + forward returns
            merged["sent_roll3"] = merged["sent_daily"].rolling(3, min_periods=1).mean()
            merged["fwd_ret1"] = merged["ret1"].shift(-1)
            merged["fwd_ret5"] = merged["ret5"].shift(-5)

            # Correlations
            def _safe_corr(a: str, b: str) -> float:
                try:
                    val = merged[[a, b]].corr().iloc[0, 1]
                    return 0.0 if math.isnan(val) else float(val)
                except Exception:
                    return 0.0
            c1 = _safe_corr("sent_daily", "fwd_ret1")
            c5 = _safe_corr("sent_roll3", "fwd_ret5")
            st.markdown(
                f"**T∆∞∆°ng quan**: c√πng ng√†y ‚Üí l·ª£i su·∫•t ng√†y k·∫ø ti·∫øp = **{c1:.3f}** ¬∑ "
                f"MA3 sentiment ‚Üí l·ª£i su·∫•t 5 ng√†y t·ªõi = **{c5:.3f}**"
            )

            # Plot: Close + sentiment
            from plotly.subplots import make_subplots
            fig1 = make_subplots(specs=[[{"secondary_y": True}]])
            fig1.add_trace(go.Scatter(x=merged["date"], y=merged["Close"], name="Close"), secondary_y=False)
            fig1.add_trace(go.Scatter(x=merged["date"], y=merged["sent_roll3"], name="Sentiment (MA3)"), secondary_y=True)
            fig1.update_yaxes(title_text="Close", secondary_y=False)
            fig1.update_yaxes(title_text="Sentiment (MA3)", secondary_y=True)
            fig1.update_layout(height=350, margin=dict(l=10, r=10, t=10, b=10))
            st.plotly_chart(fig1, use_container_width=True)

            # Scatter (no trendline dependency)
            scat = px.scatter(
                merged,
                x="sent_roll3",
                y="fwd_ret5",
                labels={"sent_roll3": "Sentiment (MA3)", "fwd_ret5": "L·ª£i su·∫•t 5 ng√†y t·ªõi"},
                title="Sentiment (MA3) vs. L·ª£i su·∫•t 5 ng√†y t·ªõi",
            )
            st.plotly_chart(scat, use_container_width=True)

        # Exports
        csv_prices = pxdf.to_csv(index=False).encode("utf-8")
        st.download_button("T·∫£i CSV gi√°", csv_prices, file_name=f"{raw_ticker}_prices.csv", mime="text/csv")
        if 'nd' in locals():
            csv_news = nd.to_csv(index=False).encode("utf-8")
            st.download_button("T·∫£i CSV tin t·ª©c", csv_news, file_name=f"{raw_ticker}_news.csv", mime="text/csv")

        with st.expander("Ngu·ªìn & m·∫πo truy v·∫•n"):
            st.markdown(
                "- Gi√°: Ngu·ªìn Vi·ªát Nam qua th∆∞ vi·ªán `vnstock` (VND, theo s√†n Vi·ªát).\n"
                "- Tin t·ª©c: Google News RSS ‚Äî th√™m t√™n DN/t·ª´ kh√≥a ‚Äòkqkd‚Äô, ‚Äòc·ªï t·ª©c‚Äô, ‚Äòtr√°i phi·∫øu‚Äô ƒë·ªÉ ch√≠nh x√°c h∆°n.\n"
                "- G·ª£i √Ω: D√πng m√£ 3 k√Ω t·ª± chu·∫©n c·ªßa s√†n VN, v√≠ d·ª• HPG, VNM, FPT, MBS..."
            )

    except Exception as e:
        st.error(f"L·ªói: {e}")

else:
    st.info("Nh·∫≠p m√£ c·ªï phi·∫øu v√† b·∫•m **Ph√¢n t√≠ch ngay** ƒë·ªÉ b·∫Øt ƒë·∫ßu. V√≠ d·ª•: HPG (H√≤a Ph√°t), VNM (Vinamilk), MSN (Masan), MBS (MB Securities).")
